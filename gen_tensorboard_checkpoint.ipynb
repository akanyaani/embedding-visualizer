{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# $ tensorboard --logdir ./word_embeddings/{model_name}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "model_names = [\"bert-base-uncased\", \"bert-base-cased\", \"bert-large-uncased\",\n",
    "               \"bert-large-cased\", \"bert-base-multilingual-uncased\", \"bert-base-multilingual-cased\",\n",
    "               \"bert-base-chinese\", \"bert-base-german-cased\", \"bert-large-uncased-whole-word-masking\",\n",
    "               \"bert-large-cased-whole-word-masking\", \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "               \"bert-large-cased-whole-word-masking-finetuned-squad\", \"bert-base-cased-finetuned-mrpc\",\n",
    "               \"bert-base-german-dbmdz-cased\", \"bert-base-german-dbmdz-uncased\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, return_tensors=\"tf\")\n",
    "    model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "    log_dir = './word_embeddings/{}/'.format(model_name)\n",
    "\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
    "        for i in range(tokenizer.vocab_size):\n",
    "            f.write(\"{}\\n\".format(tokenizer.convert_ids_to_tokens(i)))\n",
    "\n",
    "    weights = tf.Variable(model.bert.embeddings.weights[0])\n",
    "    checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "    checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "    embedding.metadata_path = 'metadata.tsv'\n",
    "    projector.visualize_embeddings(log_dir, config)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 290kB/s]\n",
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 174kB/s]\n",
      "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 769kB/s]\n",
      "Downloading: 100%|██████████| 1.47G/1.47G [05:06<00:00, 4.81MB/s]\n",
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-large-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Downloading: 100%|██████████| 762/762 [00:00<00:00, 346kB/s]\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 506kB/s]\n",
      "Downloading: 100%|██████████| 436k/436k [00:00<00:00, 930kB/s]\n",
      "Downloading:  67%|██████▋   | 982M/1.46G [04:30<02:12, 3.61MB/s]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "model.bert.embeddings.weights[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([28996, 768])"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#Position Embedding \n",
    "# weights = tf.Variable(model.bert.embeddings.weights[1])\n",
    "# checkpoint = tf.train.Checkpoint(embedding=weights)\n",
    "# checkpoint.save(os.path.join(log_dir, \"position_embedding.ckpt\"))\n",
    "\n",
    "# config = projector.ProjectorConfig()\n",
    "# embedding = config.embeddings.add()\n",
    "# embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "# embedding.metadata_path = 'metadata.tsv'\n",
    "# projector.visualize_embeddings(log_dir, config)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./bert/position_embedding.ckpt-1'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "18e501a9c9e24f2629782a91251d8fa84099fbfef4fc95fca27e0b673fccd294"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}